{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaQuest/TeachingDemo/blob/main/Template1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtQrDqGgMk0j",
        "outputId": "e4210704-7161-4090-9479-212ad2221674"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'this is a wonderful day'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Lower casing'''\n",
        "text = \"This is a WONDERFUL Day\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43sGGfUjMk0l",
        "outputId": "80f01cb3-dc22-4305-cb60-6aa8f178eec2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['This is a wonderful day', ' Today we have SENG 507 class']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Sentence Splitter'''\n",
        "text = \"This is a wonderful day. Today we have SENG 507 class\"\n",
        "\n",
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bGUVBqaMk0l",
        "outputId": "b20cc746-f43e-4e97-c621-9510b95e289a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Please', '.', 'Tokenize', 'this', 'sentence', '!', 'can', 'you', '?', '!']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Tokenization'''\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "text = \"Please. Tokenize this sentence! can you?!\"\n",
        "\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17WSF263Mk0l",
        "outputId": "e571d8f3-64c5-4af9-eec4-a3ee1dfc63de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Please', 'Tokenize', 'this', 'sentence', 'can', 'you']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''Remove punctuations'''\n",
        "import re\n",
        "new_words = []\n",
        "for word in text:\n",
        "    \n",
        "text = new_words\n",
        "text    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfPjD_GcMk0m",
        "outputId": "0799c9f8-ceb5-423c-d891-38678d1c7b6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Please', 'Tokenize', 'sentence']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "from nltk.corpus import stopwords\n",
        "new_words = []\n",
        "for word in text:\n",
        "   \n",
        "#stopwords.words('english')\n",
        "text = new_words\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOs85JbAMk0m",
        "outputId": "99c80e33-aead-401c-927f-6b2e6d3f8b61"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['pleas', 'tok', 'sent']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"Stem words in list of tokenized words\"\"\"\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "stemmer = LancasterStemmer()\n",
        "stems = []\n",
        "for word in text:\n",
        "    stem = stemmer.stem(word)\n",
        "    stems.append(stem)\n",
        "text = stems\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD0UUGqTMk0m",
        "outputId": "25c32231-4caf-4154-d275-94584cbee24a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Please', 'Tokenize', 'sentence']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = []\n",
        "for word in text:\n",
        "    \n",
        "text = lemmas\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh4qp4ItMk0n"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "TemplateSages1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}